# -*- coding: utf-8 -*-
"""sales-forecasting.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QJ_ylnmwiaGoc3RbdiyoHwrkt-bWvcPZ

# Wallmart Sales Forecasting

## 01- Import Libraries
"""

!pip install pmdarima

# Commented out IPython magic to ensure Python compatibility.
# %pip install --upgrade numpy pmdarima

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""## 02- Load dataset"""

df = pd.read_csv('Walmart_Sales.csv')
df.head()  # CPI = Consumer Price Index

"""## 03- EDA Steps"""

df.shape

df.describe()

df.info()

df.isnull().sum()

df.nunique()

"""## 04- Data Wrangling"""

df['Date'] = pd.to_datetime(df['Date'], format='mixed', dayfirst=True)

# Sort by Date (important for time-series)
df = df.sort_values('Date')

# Drop rows with NaN (due to lag features)
df = df.dropna()

"""## 05- Data Visualization"""

# 1. Sales trend over time
plt.figure(figsize=(15,6))
sns.lineplot(x='Date', y='Weekly_Sales', data=df)
plt.title('Weekly Sales Trend')

# 2. Holiday impact
sns.boxplot(x='Holiday_Flag', y='Weekly_Sales', data=df)

# 3. Correlation heatmap
corr = df[['Weekly_Sales','Temperature','Fuel_Price','CPI','Unemployment']].corr()
sns.heatmap(corr, annot=True)

# 4. Store comparison
top_stores = df.groupby('Store')['Weekly_Sales'].mean().nlargest(5)
sns.barplot(x=top_stores.index, y=top_stores.values)

"""## 06- Feature Engineering"""

# 1. Lag Features (previous week's sales)
df['lag_1'] = df.groupby('Store')['Weekly_Sales'].shift(1)  # 1-week lag
df['lag_4'] = df.groupby('Store')['Weekly_Sales'].shift(4)  # 4-week lag (monthly trend)

# 2. Rolling Statistics (moving averages)
df['rolling_mean_4'] = df.groupby('Store')['Weekly_Sales'].transform(lambda x: x.rolling(4).mean())
df['rolling_std_4'] = df.groupby('Store')['Weekly_Sales'].transform(lambda x: x.rolling(4).std())

# 3. Time-based Features
df['year'] = df['Date'].dt.year
df['month'] = df['Date'].dt.month
df['week_of_year'] = df['Date'].dt.isocalendar().week

# 4. Holiday Proximity (days before/after holiday)
# (Assuming 'Holiday_Flag' marks holiday weeks)
df['days_since_holiday'] = df.groupby('Store')['Holiday_Flag'].cumsum()

# 5. Economic Impact Features (normalize if needed)
df['fuel_price_change'] = df.groupby('Store')['Fuel_Price'].pct_change()
df['cpi_change'] = df.groupby('Store')['CPI'].pct_change()

print(df.shape)
df.head()

"""## 07- Splitting dataset into Training & Testing"""

# Split into train & test (last 12 weeks for testing)
split_date = df['Date'].max() - pd.Timedelta(weeks=12)
train = df[df['Date'] <= split_date]
test = df[df['Date'] > split_date]

# Separate features & target
X_train = train.drop(['Weekly_Sales', 'Date', 'Store'], axis=1)
y_train = train['Weekly_Sales']
X_test = test.drop(['Weekly_Sales', 'Date', 'Store'], axis=1)
y_test = test['Weekly_Sales']

"""## 08- Model"""

# # Not working in Google Colab
# # ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject

# from pmdarima.arima import auto_arima
# import numpy as np

# # Check versions to ensure compatibility
# print(f"NumPy version: {np.__version__}")
# print(f"pmdarima version: {pmdarima.__version__}")

# # Your existing code should now work
# model = auto_arima(
#     y_train,
#     seasonal=True,
#     m=52,  # Weekly seasonality (52 weeks/year)
#     suppress_warnings=True,
#     stepwise=True,
#     trace=True
# )

# # Fit the model
# model.fit(y_train)

# # Generate forecasts
# sarima_forecast = model.predict(n_periods=len(y_test))

"""Replacing the `pmdarima` implementation with `statsmodels` SARIMAX:"""

from statsmodels.tsa.statespace.sarimax import SARIMAX
from sklearn.metrics import mean_absolute_error

# 1. Train SARIMAX model (using reasonable default params)
model = SARIMAX(y_train,
               order=(1, 1, 1),            # Non-seasonal (p,d,q)
               seasonal_order=(1, 1, 1, 52), # Seasonal (P,D,Q,s) - weekly seasonality
               trend='c')                   # Constant trend term
results = model.fit(disp=False)

# 2. Generate forecasts
sarima_forecast = results.get_forecast(steps=len(y_test)).predicted_mean

# 3. Evaluate
mae = mean_absolute_error(y_test, sarima_forecast)
print(f"SARIMA MAE: {mae:,.2f}")

# View model summary
print(results.summary())

"""## 09- Facebook Prophet (with Regressors)"""

from prophet import Prophet

# Prepare data in Prophet format
prophet_df = train[['Date', 'Weekly_Sales', 'Holiday_Flag', 'Temperature', 'CPI']].rename(
    columns={'Date': 'ds', 'Weekly_Sales': 'y'}
)

# Initialize & fit model
model = Prophet(
    yearly_seasonality=True,
    weekly_seasonality=True,
    daily_seasonality=False
)

# Add regressors (external variables)
model.add_regressor('Holiday_Flag')
model.add_regressor('Temperature')
model.add_regressor('CPI')

model.fit(prophet_df)

# Create future dataframe with test data
future = test[['Date', 'Holiday_Flag', 'Temperature', 'CPI']].rename(columns={'Date': 'ds'})
prophet_forecast = model.predict(future)

"""## 10- Model Evaluation"""

from sklearn.metrics import mean_absolute_error, mean_squared_error

def evaluate(y_true, y_pred, model_name):
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100

    print(f"--- {model_name} Performance ---")
    print(f"MAE: {mae:.2f}")
    print(f"RMSE: {rmse:.2f}")
    print(f"MAPE: {mape:.2f}%")

# Evaluate SARIMA
evaluate(y_test, sarima_forecast, "SARIMA")

# Evaluate Prophet
evaluate(y_test, prophet_forecast['yhat'], "Prophet")

"""## 11- Saving model"""

# SARIMA (Pickle)
import joblib

# Save SARIMA model
joblib.dump(model, 'sarima_model.pkl')

# Load later
loaded_model = joblib.load('sarima_model.pkl')

# Prophet (JSON)
from prophet.serialize import model_to_json, model_from_json

# Save Prophet model
with open('prophet_model.json', 'w') as f:
    f.write(model_to_json(model))

# Load later
with open('prophet_model.json', 'r') as f:
    loaded_model = model_from_json(f.read())

"""## Key Findings  
1. **Prophet Outperforms SARIMA**:  
   - Achieved **45% lower MAE** (448K vs 815K) and **48% lower RMSE** (522K vs 1M) than SARIMA.  
   - Better at capturing complex patterns (holidays, external factors).  
2. **High MAPE Alert**:  
   - Both models show high error rates (~70-80%), indicating:  
     - Potential data noise or outliers.  
     - Need for better feature engineering (e.g., promotions, store events).  

### Recommended Actions  
âœ… **Adopt Prophet for Deployment**:  
   - Leverage its multivariate capabilities (holidays, temperature).  
âœ… **Improve Data Quality**:  
   - Investigate outliers (e.g., extreme sales weeks).  
   - Add features like marketing spend or local events.  
âœ… **Tune Hyperparameters**:  
   - Optimize Prophetâ€™s `changepoint_prior_scale` to reduce overfitting.  

### Next Steps  
ðŸ”§ **Model Enhancement**:  
   - Test **ensemble models** (SARIMA + Prophet) for robustness.  
ðŸ“Š **Business Integration**:  
   - Build a dashboard to track forecast vs. actuals weekly.  
ðŸ”„ **Continuous Learning**:  
   - Retrain models quarterly with new data.  

**Performance Snapshot**:  

| Model    | MAE      | RMSE     | MAPE  |  
|----------|----------|----------|-------|  
| SARIMA   | 815,781  | 1,004,033| 69.6% |  
| Prophet  | 448,627  | 522,839  | 78.5% |  

---

---
"""